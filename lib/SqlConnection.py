import os,sys
import logging
import logging.handlers
from cloghandler import ConcurrentRotatingFileHandler
from collections import deque
from sqlalchemy import Table, Column, Integer, Float, String, Boolean, MetaData, create_engine
from sqlalchemy.dialects.postgresql import ARRAY

class PipelineSqlConnection:
    """used to connect to the non-bibliographic sql table generated by AdsDataSqlSync
    
    The non-bib table contains about a dozen columns, such as refereed, reads, downloads, etc.
    The table is read-only to the pipeline and used  to populate the full Solr record.
    """

    def __init__(self,**kwargs):
        self.logger = kwargs.get('logger',None)
        if not self.logger:
            self.initializeLogging(**kwargs)

        self.uri = kwargs.get('NONBIB_SQL', 'sqlite://')  # default to in-memory database 
        self.schema = kwargs.get('SCHEMA','nonbib')
        self.meta = MetaData()
        self.logger.info('solr url: {}, schema: {}'.format(self.uri, self.schema))
        self.engine = create_engine(self.uri, echo=False)
        self.conn = self.engine.connect()

        # sql row view table
    def get_row_view_table(self, meta=None):
        if meta is None:
            meta = self.meta
        return Table('rowviewm', meta,
                     Column('bibcode', String, primary_key=True),
                     Column('id', Integer),
                     Column('authors', ARRAY(String)),
                     Column('refereed', Boolean),
                     Column('simbad_objects', ARRAY(String)),
                     Column('grants', ARRAY(String)),
                     Column('citations', ARRAY(String)),
                     Column('boost', Float),
                     Column('citation_count', Integer),
                     Column('read_count', Integer),
                     Column('norm_cites', Integer),
                     Column('readers', ARRAY(String)),
                     Column('downloads', ARRAY(Integer)),
                     Column('reads', ARRAY(Integer)),
                     Column('reference', ARRAY(String)),
                     schema=self.schema,
                     extend_existing=True)


    def getAllBibcodes(self):
        """only used by test code to verify sql table was populated with test data"""

        return_value = deque()
        row_view = self.get_row_view_table()
        s = select([row_view])
        rp = self.conn.execute(s)
        rows = rp.fetchall()

        return rows

    def getRecordsFromBibcodes(self, bibcodes, key='bibcode', op='$in'):
        """return an array of hastables with non-bib sql table for the requested bibcodes

        to search by id, use the key="id"
        the op parameter is only used during testing, it may not be a useful to application code
        """

        if not isinstance(bibcodes, list):
            bibcodes = [bibcodes]
        row_view = self.get_row_view_table()

        if op == '$in':
            s = select([row_view]).where(row_view.c[key].in_(bibcodes))
        elif op == '$nin':
            s = select([row_view]).where(row_view.c[key].notin_(bibcodes))
        else:
            raise ValueError('invalid op parameter {}, must be $in or $nin'.format(op))

        rp = self.conn.execute(s)
        rows = rp.fetchall()

        return rows


    def initializeLogging(self,**kwargs):
        logfmt = '%(levelname)s\t%(process)d [%(asctime)s]:\t%(message)s'
        datefmt= '%m/%d/%Y %H:%M:%S'
        formatter = logging.Formatter(fmt=logfmt,datefmt=datefmt)
        LOGGER = logging.getLogger('PipelineMongoConnection')
        if not LOGGER.handlers:
            default_fn = os.path.join(os.path.dirname(__file__),'..','logs','PipelineMongoConnection.log')
            fn = kwargs.get('logfile',default_fn)
            rfh = ConcurrentRotatingFileHandler(filename=fn,maxBytes=2097152,backupCount=10,mode='a') #2MB file
            rfh.setFormatter(formatter)
            ch = logging.StreamHandler() #console handler
            ch.setFormatter(formatter)
            #LOGGER.addHandler(ch)
            LOGGER.addHandler(rfh)
            LOGGER.setLevel(logging.DEBUG)
        self.logger = LOGGER

    def close(self):
        self.conn.close()

    # no need to remove from sql database?
    #def remove(self,spec_or_id=None,force=False):

    # not needed for sql
    #def initializeCollection(self,_index='bibcode',**kwargs):

    # I don't think this will be needed
    #def _getNextSequence(self,name='seq'):
    #    #Todo: Implement a collection that records deleted docs, enabling us to re-use those _ids.
    #    result = self.db['%s_seq' % self.collection].find_and_modify(
    #        query={'_id':name},
    #        update={'$inc': {'counter':1}},
    #        new=True
    #    )
    #    return result['counter']

    # this is read only for sql data
    #def upsertRecords(self,records,**kwargs):

    # need to understand fingerprinting
    # if fingerprint is in unified mongo record and passed arguments, sql connection can ignore it
    #def findNewRecords(self,records):
